{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a1",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ollema/nlp2019/blob/master/a1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwuKs_y0ANC9",
        "colab_type": "code",
        "outputId": "91ddaf9f-c3de-413f-b8a0-65d885c163ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/ollema/nlp2019/master/a1_data/wsd_train.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-09 11:59:08--  https://raw.githubusercontent.com/ollema/nlp2019/master/a1_data/wsd_train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 46435645 (44M) [text/plain]\n",
            "Saving to: ‘wsd_train.txt’\n",
            "\n",
            "wsd_train.txt       100%[===================>]  44.28M   143MB/s    in 0.3s    \n",
            "\n",
            "2019-11-09 11:59:11 (143 MB/s) - ‘wsd_train.txt’ saved [46435645/46435645]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgOUiz7HERp7",
        "colab_type": "code",
        "outputId": "7be08e18-4970-491e-dfde-43cdc9c49358",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import time\n",
        "import torchtext\n",
        "\n",
        "from collections import defaultdict, OrderedDict\n",
        "import copy\n",
        "import random\n",
        "random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina' \n",
        "plt.style.use('seaborn')\n",
        "\n",
        "\n",
        "class WSDClassifier(nn.Module):\n",
        "    \n",
        "    def __init__(self, text_field, class_field, emb_dim, hidden_size, update_pretrained=False):\n",
        "        super().__init__()        \n",
        "\n",
        "        self.n = 70\n",
        "\n",
        "        voc_size = len(text_field.vocab)\n",
        "        n_senses = len(class_field.vocab)   \n",
        "\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.dropout2d = nn.Dropout2d(0.05)\n",
        "        \n",
        "        # embedding layer.\n",
        "        self.embedding = nn.Embedding(voc_size, emb_dim)\n",
        "\n",
        "        # if we're using pre-trained embeddings, copy them into the model's embedding layer.\n",
        "        if text_field.vocab.vectors is not None:\n",
        "            self.embedding.weight = torch.nn.Parameter(text_field.vocab.vectors, requires_grad=update_pretrained)\n",
        "        \n",
        "        # bidirectional LSTM\n",
        "        self.lstm = nn.LSTM(input_size=emb_dim, hidden_size=hidden_size, bidirectional=True, num_layers=1)\n",
        "        self.num_directions = 2\n",
        "\n",
        "        # hidden layer\n",
        "        self.a = nn.Linear(2 * hidden_size, 2 * hidden_size)\n",
        "\n",
        "        # classification layers\n",
        "        self.y = nn.Linear(2 * hidden_size, n_senses)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        \n",
        "    def forward(self, texts):\n",
        "        # The words in the documents are encoded as integers. The shape of the documents\n",
        "        # tensor is (max_len, batch), where batch is the number of documents in this batch,\n",
        "        # and max_len is the maximal length of a document in the batch.\n",
        "\n",
        "        # First look up the embeddings for all the words in the documents.\n",
        "        # the shape is now (max_len, batch, emb_dim).\n",
        "        embedded = self.embedding(texts)\n",
        "\n",
        "        dropped_embedded = self.dropout2d(embedded)\n",
        "\n",
        "        # rnn_output: the outputs at all positions of the final layer\n",
        "        output, _ = self.lstm(dropped_embedded)\n",
        "        # output, _ = self.lstm(embedded)\n",
        "\n",
        "        self.batch_size = output.size()[1]\n",
        "\n",
        "        # the shape of output is (seq_len, batch, 2 * hidden_size)\n",
        "        # we select the forward and backward states at position n and concatenate them.\n",
        "        output = output.view(-1, self.batch_size, self.num_directions, self.lstm.hidden_size)\n",
        "        forward = output[70 - 1, :, 0, :]\n",
        "        backward = output[70, :, 1, :]\n",
        "        output = torch.cat([forward, backward], dim=1)\n",
        "        dropped_output = self.dropout(output)\n",
        "        \n",
        "        # apply the hidden layer and return the output.\n",
        "        hidden = self.a(dropped_output)\n",
        "\n",
        "        dropped_hidden = self.dropout(hidden)\n",
        "\n",
        "        # apply the top layer + softmax and return the output.\n",
        "        return self.softmax(self.y(dropped_hidden))\n",
        "\n",
        "\n",
        "def read_data(corpus_file, doc_start, with_padding = True):\n",
        "    \"\"\" Parses input file and returns filtered datasets for each word-type in corpus as well as\n",
        "    list of word-types found in corpus. \"\"\"\n",
        "    # Initialization\n",
        "    text = torchtext.data.Field(sequential=True, tokenize=lambda x: x.split())\n",
        "    label = torchtext.data.LabelField(is_target=True)\n",
        "    datafields = [('text', text), ('label', label)]\n",
        "    label_column = 0\n",
        "\n",
        "    if with_padding == True:\n",
        "        pad_string = '<pad>'\n",
        "        sentence_length = 140\n",
        "        half_sentence_length = int(sentence_length/2)\n",
        "        with open(corpus_file, encoding='utf-8') as f:\n",
        "            examples = []\n",
        "            for line in f:\n",
        "                columns = line.strip().split(maxsplit=doc_start)\n",
        "                position_of_wordtype = int(columns[2])\n",
        "\n",
        "                # Split the long string doc into array and extract words before the wordtype.\n",
        "                doc = columns[-1]\n",
        "                doc_string_vector = doc.split()\n",
        "                temp_pad = [pad_string for x in range(0,70)]\n",
        "\n",
        "                padded_doc = []\n",
        "                padded_doc.extend(temp_pad)\n",
        "                padded_doc.extend(doc_string_vector)\n",
        "                padded_doc.extend(temp_pad)\n",
        "\n",
        "                sliced_doc = padded_doc[position_of_wordtype:position_of_wordtype + 140]\n",
        "\n",
        "                if len(sliced_doc) != 140:\n",
        "                    print(sliced_doc)\n",
        "                    raise RuntimeError\n",
        "\n",
        "                sliced_doc = \" \".join(sliced_doc)\n",
        "                label = columns[label_column]\n",
        "\n",
        "                examples.append(torchtext.data.Example.fromlist([sliced_doc, label], datafields))\n",
        "    else:\n",
        "        with open(corpus_file, encoding='utf-8') as f:\n",
        "            examples = []\n",
        "            for line in f:\n",
        "                columns = line.strip().split(maxsplit=doc_start)\n",
        "                doc = columns[-1]\n",
        "                label = columns[label_column]\n",
        "                examples.append(torchtext.data.Example.fromlist([doc, label], datafields))\n",
        "    unfiltered_data = torchtext.data.Dataset(examples, datafields,filter_pred=None)\n",
        "\n",
        "    # Read complete dataset to get set of word-types. E.i 'keep', 'line'...\n",
        "    filter_function = None\n",
        "    word_types = set()\n",
        "    for example in unfiltered_data.examples:\n",
        "        word_types.add(example.label.split(\"%\", 1)[0])\n",
        "    word_types = list(word_types)\n",
        "\n",
        "    # Create filtered datasets for each word-type\n",
        "    filtered_datasets = OrderedDict()\n",
        "    for a_word_type in word_types:\n",
        "        filter_function = lambda ex: ex.label.split(\"%\", 1)[0] == a_word_type\n",
        "        text = torchtext.data.Field(sequential=True, tokenize=lambda x: x.split())\n",
        "        label = torchtext.data.LabelField(is_target=True)\n",
        "        datafields = [('text', text), ('label', label)]\n",
        "\n",
        "        filtered_data_set = torchtext.data.Dataset(examples, datafields, filter_pred=filter_function)\n",
        "        filtered_datasets[a_word_type] = (filtered_data_set, text, label)\n",
        "    return filtered_datasets\n",
        "\n",
        "\n",
        "def evaluate_validation(scores, loss_function, gold):\n",
        "    guesses = scores.argmax(dim=1)\n",
        "    n_correct = (guesses == gold).sum().item()\n",
        "    return n_correct, loss_function(scores, gold).item()\n",
        "\n",
        "\n",
        "use_pretrained = True\n",
        "filtered_datasets = read_data('wsd_train.txt', doc_start=4)\n",
        "\n",
        "if use_pretrained:\n",
        "    print('We are using pre-trained word embeddings.')\n",
        "else:\n",
        "    print('We are training word embeddings from scratch.')\n",
        "\n",
        "models = OrderedDict()\n",
        "max_verifications = OrderedDict()\n",
        "model_vocabs = OrderedDict()\n",
        "model_label_vocabs = OrderedDict()\n",
        "\n",
        "for word_type, filtered_dataset in filtered_datasets.items():\n",
        "    dataset = filtered_dataset[0]\n",
        "    text = filtered_dataset[1]\n",
        "    label = filtered_dataset[2]\n",
        "\n",
        "    train, valid = dataset.split([0.8, 0.2])\n",
        "\n",
        "    if use_pretrained:\n",
        "        text.build_vocab(train, vectors=\"glove.6B.100d\")\n",
        "    else:        \n",
        "        text.build_vocab(train, max_size=10000)\n",
        "    \n",
        "    model_vocabs[word_type] = text.vocab\n",
        "    label.build_vocab(train)\n",
        "    model_label_vocabs[word_type] = label.vocab\n",
        "        \n",
        "    model = WSDClassifier(text, label, emb_dim=100, hidden_size=74, update_pretrained=True)\n",
        "\n",
        "    device = 'cuda'\n",
        "    model.to(device)\n",
        "\n",
        "    train_iterator = torchtext.data.Iterator(\n",
        "        train,\n",
        "        device=device,\n",
        "        batch_size=128,\n",
        "        repeat=False,\n",
        "        train=True,\n",
        "        sort=False)\n",
        "\n",
        "    valid_iterator = torchtext.data.Iterator(\n",
        "        valid,\n",
        "        device=device,\n",
        "        batch_size=128,\n",
        "        repeat=False,\n",
        "        train=False,\n",
        "        sort=False)\n",
        "\n",
        "    loss_function = torch.nn.CrossEntropyLoss()   \n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=2, momentum=0.1) \n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.96)\n",
        "\n",
        "\n",
        "    train_batches = list(train_iterator)\n",
        "    valid_batches = list(valid_iterator)\n",
        "\n",
        "    history = defaultdict(list)\n",
        "    max_val_acc = -1\n",
        "\n",
        "    print(f\"Training {word_type}...\")\n",
        "    for i in range(50):\n",
        "        \n",
        "        t0 = time.time()\n",
        "        \n",
        "        loss_sum = 0\n",
        "        n_batches = 0\n",
        "\n",
        "        model.train()\n",
        "        \n",
        "        for batch in train_batches:\n",
        "                        \n",
        "            scores = model(batch.text)\n",
        "            loss = loss_function(scores, batch.label)\n",
        "\n",
        "            optimizer.zero_grad()            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_sum += loss.item()\n",
        "            n_batches += 1\n",
        "        \n",
        "        train_loss = loss_sum / n_batches\n",
        "        history['train_loss'].append(train_loss)\n",
        "        \n",
        "        n_correct = 0\n",
        "        n_valid = len(valid)\n",
        "        loss_sum = 0\n",
        "        n_batches = 0\n",
        "\n",
        "        model.eval()\n",
        "        \n",
        "        for batch in valid_batches:\n",
        "            scores = model(batch.text)\n",
        "            n_corr_batch, loss_batch = evaluate_validation(scores, loss_function, batch.label)\n",
        "            loss_sum += loss_batch\n",
        "            n_correct += n_corr_batch\n",
        "            n_batches += 1\n",
        "        val_acc = n_correct / n_valid\n",
        "        val_loss = loss_sum / n_batches\n",
        "\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        if val_acc > max_val_acc:\n",
        "            max_val_acc = val_acc\n",
        "            models[word_type] = copy.deepcopy(model)\n",
        "            max_verifications[word_type] = max_val_acc\n",
        "        \n",
        "        scheduler.step()\n",
        "\n",
        "        t1 = time.time()\n",
        "\n",
        "        if (i+1) % 10 == 0:\n",
        "            print(f'Epoch {i+1}: train loss = {train_loss:.4f}, val loss = {val_loss:.4f}, val acc: {val_acc:.4f}, time = {t1-t0:.4f}, lr = {optimizer.param_groups[0][\"lr\"]}')\n",
        "\n",
        "    plt.plot(history['train_loss'])\n",
        "    plt.plot(history['val_loss'])\n",
        "    plt.plot(history['val_acc'])\n",
        "    plt.legend(['training loss', 'validation loss', 'validation accuracy'])\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r.vector_cache/glove.6B.zip: 0.00B [00:00, ?B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "We are using pre-trained word embeddings.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip:  26%|██▋       | 227M/862M [01:28<10:10:09, 17.4kB/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLZ5QXRP-lYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg = sum(max_verifications.values())/30\n",
        "\n",
        "fig=plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
        "plt.bar(list(max_verifications.keys()), max_verifications.values(), color='g')\n",
        "x = [i for i in range(0,30)]\n",
        "y = [0.3 for i in range(0,30)]\n",
        "plt.plot(x,y,'b--')\n",
        "x = [i for i in range(0,30)]\n",
        "y = [avg for i in range(0,30)]\n",
        "plt.plot(x,y,'k-')\n",
        "plt.tick_params(axis='both', which='major', labelsize=30) \n",
        "plt.xticks(rotation='vertical')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkeFL6epA-Zy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://raw.githubusercontent.com/peterSvenningsson/NLP_documents_/master/wsd_test_blind.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4JBS7JWA48_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_test_data(corpus_file, doc_start):\n",
        "    TEXT = torchtext.data.Field(sequential=True, tokenize=lambda x: x.split())\n",
        "    WORDTYPE = torchtext.data.Field()\n",
        "    datafields = [('text', TEXT), ('wordtype', WORDTYPE)]\n",
        "    pad_string = '<pad>'\n",
        "    sentence_length = 140\n",
        "    half_sentence_length = int(sentence_length/2)\n",
        "    with open(corpus_file, encoding='utf-8') as f:\n",
        "        examples = []\n",
        "        for line in f:\n",
        "            columns = line.strip().split(maxsplit=doc_start)\n",
        "            position_of_wordtype = int(columns[2])\n",
        "\n",
        "            # Split the long string doc into array and extract words before the wordtype.\n",
        "            doc = columns[-1]\n",
        "            doc_string_vector = doc.split()\n",
        "            temp_pad = [pad_string for x in range(0,70)]\n",
        "\n",
        "            padded_doc = []\n",
        "            padded_doc.extend(temp_pad)\n",
        "            padded_doc.extend(doc_string_vector)\n",
        "            padded_doc.extend(temp_pad)\n",
        "\n",
        "            sliced_doc = padded_doc[position_of_wordtype:position_of_wordtype + 140]\n",
        "\n",
        "            if len(sliced_doc) != 140:\n",
        "                print(sliced_doc)\n",
        "                raise RuntimeError\n",
        "\n",
        "            sliced_doc = \" \".join(sliced_doc)\n",
        "\n",
        "            wordtype = columns[1].split('.')[0]\n",
        "            examples.append(torchtext.data.Example.fromlist([sliced_doc, wordtype], datafields))\n",
        "    dataset = torchtext.data.Dataset(examples, datafields)\n",
        "    return (dataset, TEXT, WORDTYPE)\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv_bmxXiBx89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataset = read_test_data('wsd_test_blind.txt', doc_start=4)\n",
        "for example in test_dataset[0].examples:\n",
        "    test_example_string = example.text\n",
        "    example_wordtype = example.wordtype[0]\n",
        "\n",
        "    # Load vocabs\n",
        "    example_vocab = model_vocabs[example_wordtype]\n",
        "    example_label_vocabs = model_label_vocabs[example_wordtype]\n",
        "\n",
        "    # Encode string\n",
        "    encoded_example = torch.tensor([example_vocab.stoi[x] for x in test_example_string],device=device, requires_grad = False)\n",
        "\n",
        "    # Load correct model\n",
        "    model = models[example_wordtype]\n",
        "\n",
        "    # Get prediction\n",
        "    scores = model(encoded_example)\n",
        "    prediction = scores.argmax(dim=0)\n",
        "    \n",
        "    # Decode prediction\n",
        "    decoded_prediction = example_label_vocabs.itos[prediction]\n",
        "    print(decoded_prediction)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}