{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a1",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ollema/nlp2019/blob/master/a1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwuKs_y0ANC9",
        "colab_type": "code",
        "outputId": "3e16e1fc-f8bd-49b6-de34-ba6cd820e911",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/ollema/nlp2019/master/a1_data/wsd_train.txt"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-06 16:23:23--  https://raw.githubusercontent.com/ollema/nlp2019/master/a1_data/wsd_train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 46435645 (44M) [text/plain]\n",
            "Saving to: ‘wsd_train.txt.25’\n",
            "\n",
            "\rwsd_train.txt.25      0%[                    ]       0  --.-KB/s               \rwsd_train.txt.25    100%[===================>]  44.28M   228MB/s    in 0.2s    \n",
            "\n",
            "2019-11-06 16:23:23 (228 MB/s) - ‘wsd_train.txt.25’ saved [46435645/46435645]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgOUiz7HERp7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import time\n",
        "import torchtext\n",
        "\n",
        "import random\n",
        "random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "\n",
        "class WSDClassifier(nn.Module):\n",
        "    \n",
        "    def __init__(self, text_field, class_field, emb_dim, hidden_size, update_pretrained=False):\n",
        "        super().__init__()        \n",
        "\n",
        "        self.n = 70\n",
        "        self.batch_size = 128\n",
        "\n",
        "        voc_size = len(text_field.vocab)\n",
        "        n_senses = len(class_field.vocab)   \n",
        "        \n",
        "        # embedding layer.\n",
        "        self.embedding = nn.Embedding(voc_size, emb_dim)\n",
        "\n",
        "        # if we're using pre-trained embeddings, copy them into the model's embedding layer.\n",
        "        if text_field.vocab.vectors is not None:\n",
        "            self.embedding.weight = torch.nn.Parameter(text_field.vocab.vectors, requires_grad=update_pretrained)\n",
        "        \n",
        "        # bidirectional LSTM\n",
        "        self.lstm = nn.LSTM(input_size=emb_dim, hidden_size=hidden_size, bidirectional=True, num_layers=1)\n",
        "        self.num_directions = 2\n",
        "\n",
        "        # hidden layer\n",
        "        self.a = nn.Linear(2 * hidden_size, 2 * hidden_size)\n",
        "\n",
        "        # classification layer\n",
        "        self.y = nn.Softmax(nn.Linear(2 * hidden_size, n_senses))\n",
        "\n",
        "        \n",
        "    def forward(self, texts):\n",
        "        # The words in the documents are encoded as integers. The shape of the documents\n",
        "        # tensor is (max_len, batch), where batch is the number of documents in this batch,\n",
        "        # and max_len is the maximal length of a document in the batch.\n",
        "\n",
        "        # First look up the embeddings for all the words in the documents.\n",
        "        # The shape is now (max_len, batch, emb_dim).\n",
        "        print(\"texts size:\", texts.size())\n",
        "        embedded = self.embedding(texts)\n",
        "        \n",
        "        # rnn_output: the outputs at all positions of the final layer\n",
        "        print(\"embedded size:\", embedded.size())\n",
        "        output, _ = self.lstm(embedded)\n",
        "        print(\"output size:\", output.size())\n",
        "\n",
        "        # the shape of output is (seq_len, batch, 2 * hidden_size)\n",
        "        # we select the forward and backward states at position n and concatenate them.\n",
        "        output = output.view(-1, self.batch_size, self.num_directions, self.lstm.hidden_size)\n",
        "        print(\"output size:\", output.size())\n",
        "\n",
        "        forward = output[70 - 1, :, 0, :]\n",
        "        backward = output[70, :, 1, :]\n",
        "\n",
        "        print(forward.size())\n",
        "        print(backward.size())\n",
        "\n",
        "        raise RuntimeError\n",
        "\n",
        "        top_both = torch.cat([top_forward, top_backward], dim=1)\n",
        "        \n",
        "        # apply the hidden layer and return the output.\n",
        "        hidden = self.a(top_both)\n",
        "\n",
        "        # apply the top layer + softmax and return the output.\n",
        "        return self.y(top_both)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkCAAc0_s3WH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data(corpus_file, doc_start, with_padding = True):\n",
        "    \"\"\" Parses input file and returns filtered datasets for each word-type in corpus as well as\n",
        "    list of word-types found in corpus. \"\"\"\n",
        "    # Initialization\n",
        "    text = torchtext.data.Field(sequential=True, tokenize=lambda x: x.split())\n",
        "    label = torchtext.data.LabelField(is_target=True)\n",
        "    datafields = [('text', text), ('label', label)]\n",
        "    label_column = 0\n",
        "\n",
        "    if with_padding == True:\n",
        "        pad_string = '<pad>'\n",
        "        sentence_length = 140\n",
        "        half_sentence_length = int(sentence_length/2)\n",
        "        with open(corpus_file, encoding='utf-8') as f:\n",
        "            examples = []\n",
        "            for line in f:\n",
        "                columns = line.strip().split(maxsplit=doc_start)\n",
        "                position_of_wordtype = int(columns[2])\n",
        "\n",
        "                # Split the long string doc into array and extract words before the wordtype.\n",
        "                doc = columns[-1]\n",
        "                doc_string_vector = doc.split()\n",
        "                temp_pad = [pad_string for x in range(0,70)]\n",
        "\n",
        "                padded_doc = []\n",
        "                padded_doc.extend(temp_pad)\n",
        "                padded_doc.extend(doc_string_vector)\n",
        "                padded_doc.extend(temp_pad)\n",
        "\n",
        "                sliced_doc = padded_doc[position_of_wordtype:position_of_wordtype + 140]\n",
        "\n",
        "                if len(sliced_doc) != 140:\n",
        "                    print(sliced_doc)\n",
        "                    raise RuntimeError\n",
        "\n",
        "                sliced_doc = \" \".join(sliced_doc)\n",
        "                label = columns[label_column]\n",
        "\n",
        "                examples.append(torchtext.data.Example.fromlist([sliced_doc, label], datafields))\n",
        "    else:\n",
        "        with open(corpus_file, encoding='utf-8') as f:\n",
        "            examples = []\n",
        "            for line in f:\n",
        "                columns = line.strip().split(maxsplit=doc_start)\n",
        "                doc = columns[-1]\n",
        "                label = columns[label_column]\n",
        "                examples.append(torchtext.data.Example.fromlist([doc, label], datafields))\n",
        "    unfiltered_data = torchtext.data.Dataset(examples, datafields,filter_pred=None)\n",
        "\n",
        "    # Read complete dataset to get set of word-types. E.i 'keep', 'line'...\n",
        "    filter_function = None\n",
        "    word_types = set()\n",
        "    for example in unfiltered_data.examples:\n",
        "        word_types.add(example.label.split(\"%\", 1)[0])\n",
        "    word_types = list(word_types)\n",
        "\n",
        "    # Create filtered datasets for each word-type\n",
        "    filtered_datasets = {}\n",
        "    for a_word_type in word_types:\n",
        "        filter_function = lambda ex: ex.label.split(\"%\", 1)[0] == a_word_type\n",
        "        text = torchtext.data.Field(sequential=True, tokenize=lambda x: x.split())\n",
        "        label = torchtext.data.LabelField(is_target=True)\n",
        "        datafields = [('text', text), ('label', label)]\n",
        "\n",
        "        filtered_data_set = torchtext.data.Dataset(examples, datafields, filter_pred=filter_function)\n",
        "        filtered_datasets[a_word_type] = (filtered_data_set, text, label)\n",
        "    return filtered_datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSEVLoZGtJVn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "outputId": "85123dca-749e-4b41-d64d-c5039fcc1d23"
      },
      "source": [
        "use_pretrained = False\n",
        "from collections import defaultdict\n",
        "\n",
        "filtered_datasets = read_data('wsd_train.txt', doc_start=4)\n",
        "\n",
        "for word_type, filtered_dataset in filtered_datasets.items():\n",
        "    dataset = filtered_dataset[0]\n",
        "    text = filtered_dataset[1]\n",
        "    label = filtered_dataset[2]\n",
        "\n",
        "    train, valid = dataset.split([0.8, 0.2])\n",
        "\n",
        "    if use_pretrained:\n",
        "        print('We are using pre-trained word embeddings.')\n",
        "        text.build_vocab(train, vectors=\"glove.6B.100d\")\n",
        "    else:        \n",
        "        print('We are training word embeddings from scratch.')\n",
        "        text.build_vocab(train, max_size=10000)\n",
        "    \n",
        "    label.build_vocab(train)\n",
        "        \n",
        "    model = WSDClassifier(text, label, emb_dim=100, hidden_size=74, update_pretrained=True)\n",
        "\n",
        "    device = 'cuda'\n",
        "    model.to(device)\n",
        "\n",
        "    # example = train.examples.pop()\n",
        "    # print(len(example.text))\n",
        "\n",
        "    # raise RuntimeError\n",
        "\n",
        "    train_iterator = torchtext.data.Iterator(\n",
        "        train,\n",
        "        device=device,\n",
        "        batch_size=128,\n",
        "        repeat=False,\n",
        "        train=True,\n",
        "        sort=False)\n",
        "\n",
        "    valid_iterator = torchtext.data.Iterator(\n",
        "        valid,\n",
        "        device=device,\n",
        "        batch_size=128,\n",
        "        repeat=False,\n",
        "        train=False,\n",
        "        sort=False)\n",
        "\n",
        "    loss_function = torch.nn.CrossEntropyLoss()   \n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=2) \n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.96)\n",
        "\n",
        "\n",
        "    train_batches = list(train_iterator)\n",
        "    valid_batches = list(valid_iterator)\n",
        "\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for i in range(25):\n",
        "        \n",
        "        t0 = time.time()\n",
        "        \n",
        "        loss_sum = 0\n",
        "        n_batches = 0\n",
        "\n",
        "        model.train()\n",
        "        \n",
        "        for batch in train_batches:\n",
        "                        \n",
        "            scores = model(batch.text)\n",
        "            loss = loss_function(scores, batch.label)\n",
        "\n",
        "            optimizer.zero_grad()            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_sum += loss.item()\n",
        "            n_batches += 1\n",
        "        \n",
        "        train_loss = loss_sum / n_batches\n",
        "        history['train_loss'].append(train_loss)\n",
        "        \n",
        "        n_correct = 0\n",
        "        n_valid = len(valid)\n",
        "        loss_sum = 0\n",
        "        n_batches = 0\n",
        "\n",
        "        model.eval()\n",
        "        \n",
        "        for batch in valid_batches:\n",
        "            scores = model(batch.text)\n",
        "            n_corr_batch, loss_batch = evaluate_validation(scores, loss_function, batch.label)\n",
        "            loss_sum += loss_batch\n",
        "            n_correct += n_corr_batch\n",
        "            n_batches += 1\n",
        "        val_acc = n_correct / n_valid\n",
        "        val_loss = loss_sum / n_batches\n",
        "\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)        \n",
        "        \n",
        "        scheduler.step()\n",
        "\n",
        "        t1 = time.time()\n",
        "\n",
        "        print(f'Epoch {i+1}: train loss = {train_loss:.4f}, val loss = {val_loss:.4f}, val acc: {val_acc:.4f}, time = {t1-t0:.4f}, lr = {optimizer.param_groups[0][\"lr\"]}')\n",
        "\n",
        "        break\n",
        "\n",
        "    plt.plot(history['train_loss'])\n",
        "    plt.plot(history['val_loss'])\n",
        "    plt.plot(history['val_acc'])\n",
        "    plt.legend(['training loss', 'validation loss', 'validation accuracy'])"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We are training word embeddings from scratch.\n",
            "texts size: torch.Size([140, 128])\n",
            "embedded size: torch.Size([140, 128, 100])\n",
            "output size: torch.Size([140, 128, 148])\n",
            "output size: torch.Size([140, 128, 2, 74])\n",
            "torch.Size([128, 74])\n",
            "torch.Size([128, 74])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-122-7918f7a1b5b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-120-3ae9dda49844>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mtop_both\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtop_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_backward\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: "
          ]
        }
      ]
    }
  ]
}